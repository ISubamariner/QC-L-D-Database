{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating the Quezon City Government Leadership and Development Database System for the Automated Production of the Trainings Scorecard\n",
    "### by Ian Salig U. Batangan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Scope\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an internship output for the Strategic Human Resource Unit(SHRU) of the Quezon City Government.\n",
    "\n",
    "For compliance of the status of PRIME HRM by the Civil Service Commission(CSC), SHRU needs to update their data monitoring for the current feedback system for the trainings they dole out, to upskill and certify current employees, in order to efficiently monitor the effect of the trainings to the employees and also measure the metrics needed for the status compliance.\n",
    "\n",
    "The challenge is to design an add-on the current system that can automate the cleaning of data and to create a google sheet that can recall and store the needed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHRU's current system is heavily based on google sheets, primarily since it is a free alternative to a cloud database where a person without any coding expertise can manipulate and collate data, and can easily share with another individual without a paywall.\n",
    "\n",
    "The primary system used is done via downloading the training registration, in xlxs format, and post-training, in csv format, files on the participants from a third-party application. It is then collated manually and inserted to a data visualization google sheet. This does not accurately show when the data is looked as a whole, and the visualizations can only show per training and done by manually creating pivot tables. \n",
    "\n",
    "In order to streamline the process and reduce the human error, a two-pronged approach is created. A python file that will clean and collate the data, and an automated Google Sheet that can sort which data to show relevant data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data should be downloaded from the third party application, then renamed to their specified training code, batch number, and what type of data (registration or post training).\n",
    "\n",
    "|  | Format | Example|\n",
    "|:--------:|:--------:|:--------:|\n",
    "|  Registration Data  |  [TRAINING CODE][BATCH NUMBER]_Reg.xlsx  |  WETCT3_Reg.xlsx  |\n",
    "|  Post-Training Data |  [TRAINING CODE][BATCH NUMBER]_Post.csv   |  WETCT3_Post.csv    |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is then placed in a folder containing the python program and a folder named \"MergedFiles2\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Code\n",
    "\n",
    "Since the datasets can contain up to 500 participants per training, manual encoding/collating of the data is gruelling and time consuming. A python code is created to be able to select and collate the data needed for data visualizaation. The Python Code is also created in Python3.13 environment but no special recent update is used so it should work at any Python3 environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code starts with an inbuilt installer of the 3rd party libraries, not in the Python Standard Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pip(): #install needed 3rd party libraries\n",
    "    import sys\n",
    "    import subprocess\n",
    "    # implement pip as a subprocess:\n",
    "    packagename=('pandas','pathlib','openpyxl')\n",
    "    for i in packagename:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', i])\n",
    "    # process output with an API in the subprocess module:\n",
    "    reqs = subprocess.check_output([sys.executable, '-m', 'pip', 'freeze'])\n",
    "    installed_packages = [r.decode().split('==')[0] for r in reqs.split()]\n",
    "    print(installed_packages)\n",
    "#pip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The libraries used within the python code is then imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for data manipulation library, also needs openpyxl sub library of pandas, pandas library\n",
    "import pandas as pd\n",
    "\n",
    "#string manipulation, regular expressions library\n",
    "import re\n",
    "\n",
    "#for finding file path, glob library and pathlib library\n",
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Merge Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its primary purpose is to left join the post training dataset and the relevant data from the registrants dataset. The function takes in two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(variable_name,root_path):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path of the file to be used is defined by the root_path variable and the variable_name by using f-string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determining path\n",
    "path_reg = f\"{root_path}/{variable_name}_Reg.xlsx\"\n",
    "path_post = f\"{root_path}/{variable_name}_Post.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relevant data is then turned into a dataframe so that data manipulation using pandas is possible. THe registration dataset is from a .xlsx file and the post training dataset is from a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads raw excel and csv\n",
    "df_reg = pd.read_excel(path_reg)\n",
    "df_post = pd.read_csv(path_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent duplicate columns with different capitalizations all the columns are uppercased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg.columns = map(str.upper, df_reg.columns)\n",
    "df_post.columns = map(str.upper, df_post.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns is then made into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg_columns = df_reg.columns\n",
    "df_post_columns = df_post.columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some trainings that did not require the participants assessment scores. But since python returns an error value when we try to pull this data, we tag the dataset if they do not have an assessment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "#checking the list if the data set has assessment\n",
    "if 'PRE-ASSESSMENT TOTAL' not in df_reg_columns or 'QTOTAL' not in df_post_columns:check_pre=1\n",
    "else: check_pre= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is cluttered with uneeded data, the extrenous data is sorted out of the list of columns using list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mass removal of extrenous data\n",
    "df_reg_cleaned = [e for e in df_reg_columns if \"PRE-ASSESSMENT\" not in e and 'EMAIL ADDRESS' not in e and  \\\n",
    "                          'NICKNAME' not in e and 'ENDORSEMENT LETTER' not in e and 'CSC UPLOADED' not in e \\\n",
    "                        and 'DATE ANSWERED' not in e and 'EXPECTED OUTCOMES'not in e and 'DATA PRIVACY CONSENT' \\\n",
    "                            not in e and 'CONTACT NUMBER' not in e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using re library to exlude columns with Q in name in specific cases\n",
    "df_post_cleaned = [e for e in df_post_columns if not re.match(re.compile('Q.+-' ) , e) and not re.match(re.compile('Q..' ) , e)and not re.match(re.compile('Q.' ) , e) \\\n",
    "                        and 'EMAIL ADDRESS' not in e and 'NICKNAME' not in e and \\\n",
    "                        'ENDORSEMENT LETTER' not in e and 'CSC UPLOADED' not in e and 'DATE ANSWERED' not in e and \\\n",
    "                            'EXPECTED OUTCOMES'not in e and 'DATA PRIVACY CONSENT'not in e and 'CONTACT NUMBER' not in e]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the previous step was a total wipe we add the crucial assesment scoring data back into the list of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adds back the needed column since mass deletion was done previously\n",
    "if check_pre == 0:\n",
    "    df_reg_cleaned = df_reg_cleaned+['PRE-ASSESSMENT TOTAL']\n",
    "    df_post_cleaned = df_post_cleaned+ ['QTOTAL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since max score is not specified, the number of columns with \"Q...\" is counted and the number of elements is the number of questions therefore the max available score since the scores are all weighted equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #finding maximum score\n",
    "Qmax_list = [e for e in df_post_columns if re.match(re.compile('Q.+-' ) , e) ]\n",
    "max_score=len(Qmax_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the column names cleaned, the relevant data can be extracted from the data frames. The max score and training code of the data is also added to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieving specific column names\n",
    "df_reg = df_reg.loc[:,df_reg_cleaned]\n",
    "df_post = df_post.loc[:,df_post_cleaned]\n",
    "df_post= df_post.assign(Maximum_Assesment_Score=max_score,Training_Code=variable_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joining of the datasets is done with the merge command of pandas. Since some datasets have full names instead of spearated names, an if-else case is created to accomodate different types of datasets. The joining is done with the names, designation, section department, employment type, and sex of the training participant. The dataset sometimes mutates so the drop duplicates command is done after. It is then saved at MergedFiles2 as a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Left Join of post and reg data and drops duplicates\n",
    "        #checking if data set has full name if not full name is created\n",
    "if 'FULL NAME' in df_post_columns and 'FULL NAME' in df_reg_columns:\n",
    "        df_merge= df_reg.merge(df_post,left_on=['FULL NAME',\"DESIGNATION/POSITION\",\"DIVISION/ SECTION\",\\\n",
    "                                                    'DEPARTMENT/ OFFICE/ UNIT/ TASK FORCE','EMPLOYMENT TYPE','SEX'],\\\n",
    "                                                        right_on=['FULL NAME',\"DESIGNATION/POSITION\",\"DIVISION/ SECTION\",\\\n",
    "                                                    'DEPARTMENT/ OFFICE/ UNIT/ TASK FORCE','EMPLOYMENT TYPE','SEX']).drop_duplicates()\n",
    "else:\n",
    "        df_merge= df_reg.merge(df_post,left_on=[\"LAST NAME\",\"FIRST NAME\",\"MIDDLE INITIAL\",\"DESIGNATION/POSITION\",\\\n",
    "                                                    \"DIVISION/ SECTION\",'DEPARTMENT/ OFFICE/ UNIT/ TASK FORCE','EMPLOYMENT TYPE','SEX'],\\\n",
    "                                                            right_on=[\"LAST NAME\",\"FIRST NAME\",\"MIDDLE INITIAL\",\"DESIGNATION/POSITION\",\"DIVISION/ SECTION\",\\\n",
    "                                                        'DEPARTMENT/ OFFICE/ UNIT/ TASK FORCE','EMPLOYMENT TYPE','SEX']).drop_duplicates()\n",
    "        #generate Full Name\n",
    "        df_merge['FULL NAME'] = df_merge[\"FIRST NAME\"]+ \" \" + df_merge[\"MIDDLE INITIAL\"] + \" \"+ df_merge[\"LAST NAME\"]\n",
    "        \n",
    "    #saves the merged data to a csv file\n",
    "        \n",
    "df_merge.to_csv(f\"{root_path}/MergedFiles/{variable_name}_merged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the path of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding file path\n",
    "root_path=Path.cwd()\n",
    "print(f'File path is {root_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the file names and glob we can define the number of trainings and a list of the trainings is created by looking at the more crucial post-training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks current folder for all the trainings with post assesment and makes a list of the unique trainings\n",
    "path_list=glob.glob(f\"*_Post.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An empty list and an empty dataframe is then created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list=[]\n",
    "main_df=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating over the list called path_list and removing the last 9 characters in order to isolate the training name and batch number and the number of elements in the list determines the number of trainings in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in path_list: #change into list comprehension\n",
    "    training_list.append(name[:-9:])\n",
    "total_files=len(training_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating over the training_list as the input for the custom merge function to left join the post database and the registration database where it is temporarily stored and concatinated to the main dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #uses list of trainings to look for the file to merge\n",
    "i=0\n",
    "for name in training_list:\n",
    "    i+=1\n",
    "    temp_df=merge(name,root_path) #temporary dataframe\n",
    "    main_df=pd.concat([main_df,temp_df]) #concatenates the data\n",
    "    print(f'{name} is merged, with a shape {temp_df.shape} \\n {i} out of {total_files}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we clean the presentation of the data for the users ease of use and easier data quality checking. First the list of available columns is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-arranging the Data Columns\n",
    "main_df_columns= main_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolating the relevant participant data. So that it can be seen at the start of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df_columns_tag=[\"ID\",'Training_Code',\"FULL NAME\",\"LAST NAME\",\"FIRST NAME\",\"MIDDLE INITIAL\",\\\n",
    "                          \"DESIGNATION/POSITION\",\"DIVISION/ SECTION\",'DEPARTMENT/ OFFICE/ UNIT/ TASK FORCE',\\\n",
    "                            'EMPLOYMENT TYPE','SEX','PRE-ASSESSMENT TOTAL', 'QTOTAL', 'Maximum_Assesment_Score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining data columns can be subdivided into 7 categories(Program Design, Training Materials, Logistics, Expectations, Administration, Comments, and Facilitator), using list comprehension and regular expressions library on the column list, a list is created for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df_columns_design=[e for e in main_df_columns if re.match(re.compile('PROGRAM DESIGN.+' ) , e) ]\n",
    "    \n",
    "main_df_columns_trainingmat=[e for e in main_df_columns if re.match(re.compile('TRAINING.+' ) , e)]\n",
    "\n",
    "main_df_columns_logistics=[e for e in main_df_columns if re.match(re.compile('LOGISTICS.+' ) , e)]\n",
    "\n",
    "main_df_columns_expectations=[e for e in main_df_columns if re.match(re.compile('EXPECTATION.+' ) , e)]\n",
    "    \n",
    "main_df_columns_administration=[e for e in main_df_columns if re.match(re.compile('ADMINISTRATION.+' ) , e)]\n",
    "\n",
    "main_df_columns_comments=[e for e in main_df_columns if re.match(re.compile('COMMENT.+' ) , e)]\n",
    "\n",
    "main_df_columns_facilitators=[e for e in main_df_columns if re.match(re.compile('FACILITATOR.+' ) , e)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ordered list is created by joining the different categories. Then the extra columns is also grouped so that no data will be lost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df_ordered_list= main_df_columns_tag + main_df_columns_design + main_df_columns_trainingmat + main_df_columns_logistics + main_df_columns_expectations + main_df_columns_administration + main_df_columns_comments + main_df_columns_facilitators\n",
    "    \n",
    "main_df_columns_others=[e for e in main_df_columns if e not in main_df_ordered_list]\n",
    "\n",
    "main_df_ordered_list= main_df_columns_tag + main_df_columns_design + main_df_columns_trainingmat + main_df_columns_logistics + main_df_columns_expectations + main_df_columns_administration + main_df_columns_comments + main_df_columns_facilitators + main_df_columns_others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ordered list, the columns of the main dataframe is rearranged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df= main_df[main_df_ordered_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the user is updated on the shape of the main dataframe and is exported to the MergeFiles2 folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((main_df_ordered_list+main_df_columns_others))\n",
    "\n",
    "    #feedbacka\n",
    "print('Main DataFrame is Updated')\n",
    "print(f'The total data shape is {main_df.shape}')\n",
    "main_df.to_csv(f\"{root_path}/MergedFiles/AllConcat.csv\") \n",
    "\n",
    "main_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that is required to present is for each individual training, and all the trainings in total. Since the sub-categories varies for each training and to future proof the google sheets to an ever-expanding list of subcategories, the Power Query call is based on a dynamic list instead of calling using hard coordinates/columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Filtering\n",
    "The All_Concat file is copy and pasted to the google sheets. Basic Power Query is used with a reference to the inbuilt dashboard in order to select which data to show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [],
   "source": [
    "=IF('DASHBOARD V2'!B3=\"All\",QUERY(AllConcat,\"SELECT *\"),QUERY(AllConcat,\"SELECT * WHERE C='\"&'DASHBOARD V2'!B3&\"'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different sheet draws from the selected training data and filters out columns that dont have data. The first formula just draws the participant data, the second formula draws the filtered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [],
   "source": [
    "=QUERY(PullSpecificTraining!A1:O)\n",
    "\n",
    "=FILTER(PullSpecificTraining!P1:HG,LEN(TRIM(QUERY(PullSpecificTraining!P2:HG,,9^9))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this filter sheet a columns list made and transposed for easier calling later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [],
   "source": [
    "=unique(TRANSPOSE(Filtered!1:1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Gain is a metric using the assessment scores of the participants. Extracting the Max Score, Pre-assessment, and Post-Assesment scores. The Learning Gain is calculated using a CSC's equation.\n",
    "\n",
    "$$\n",
    "    Learning Gain = 100\\% * \\frac{PostTest Score - PreTest Score}{Maximum Score - PreTest Score}\n",
    "$$\n",
    "\n",
    "When the equation returns an undefined answer it is equated to 0%.\n",
    "\n",
    "Since the Assessment Scores are right after the participant information, the data can be extracted by column coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [],
   "source": [
    "=QUERY(LearningGain,\"Select ((Col2-Col1)/(Col3-Col1))*100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the data in gsheets returns blank since an undefined answer is not returned. To average the score, the unique ids are counted and used as the denominator to the sum of all the participants learning gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the other categories, a dynamic list is created using the columns list created early on. This is then sorted by the use of keywords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [],
   "source": [
    "Example:\n",
    "=QUERY(Columns, \"Select Col1 WHERE Col1 CONTAINS 'PROGRAM DESIGN'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamic list is then used as an array in where the individual elements in the dynamic list is used as column names to be extracted and as a added measure it also checks if the rows after the header contains an element or value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [],
   "source": [
    "=QUERY(Filtered!A1:GY, \"SELECT \"&TEXTJOIN(\", \", TRUE, ARRAYFORMULA(\"Col\"&XMATCH(TOCOL(A5:A, 1), Filtered!A1:GY1)))&\" WHERE Col1 IS NOT NULL\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the count of elements/votes of each subcategory varies, the average score given by the subcategory is given weight by multiplying the average score by the quotient of total count of the votes and the unique id count, turning it into a weighted average.\n",
    "$$\n",
    "Weight = \\frac{Total Score Count}{Total Unique ID Count}\n",
    "$$\n",
    "$$\n",
    "Weighted Average = {Average Score} * Weight\n",
    "$$\n",
    "The sum of the Weighted Average of each subcategory is then divided by the sum of the Total Weight of each subcategory in order to get an accurate Categorical Average. The weighted average in each category is then summed and divided by the total weight of all the subcategories. To show a good approximate of the average of all the scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dashboard\n",
    "The dashboard is a basic scorecard where the relevant is just drawn from the several sheets in the spreadsheet using query. Histogram of the calculated Learning Gain of the Participant is also shown to show where the data skew happens, and to be able to visualize the where the partipants score are placed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dashboard](/Users/internship/Downloads/AllDupe/Dashboard2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FlowChart](/Users/internship/Downloads/AllDupe/FlowChart1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database system hinges on the constant update by the HRMD personel but by automating the calculations and data cleaning the job task can be done with a few clicks. Any other monitoring, i.e. searching for the amount of participants for each department,is easier because all of the data needed is in one place. Any other visualizations can be done easily since the data is automatically sorted within the google sheet and given a metric based on the count of the score versus the count of the participants. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the python code is heavily reliant on lists, there is significant room for optimization if static tuples is used to store the dataframes and the list are deleted. The automation of importing of the AllConcat data to the google sheets was also tried  but the sheer amount of data being written in one instance leads to the API timing out since a free google cloud account is being used. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
